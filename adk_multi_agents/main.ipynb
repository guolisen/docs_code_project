{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent System with Google ADK\n",
    "\n",
    "This notebook demonstrates how to create a multi-agent system using Google's Agent Development Kit (ADK). The system consists of:\n",
    "\n",
    "1. A Billing Agent that handles billing and payment-related inquiries\n",
    "2. A Support Agent that provides technical support\n",
    "3. A Coordinator Agent that routes user requests to the appropriate specialized agent\n",
    "\n",
    "The notebook shows how to set up these agents, create a runner, and process user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, we need to import the necessary libraries from Google ADK and configure the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.artifacts import InMemoryArtifactService\n",
    "from google.adk.memory.in_memory_memory_service import InMemoryMemoryService\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "import logging\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=\"\")\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
    "MODEL_GPT_4O = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"<<MultiAgentTest>>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Specialized Sub-Agents\n",
    "\n",
    "We'll create two specialized agents:\n",
    "1. A Billing Agent for handling payment-related inquiries\n",
    "2. A Support Agent for handling technical support requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specialized sub-agents\n",
    "billing_agent = LlmAgent(\n",
    "    name=\"Billing\",\n",
    "    model=LiteLlm(model=MODEL_GPT_4O),\n",
    "    instruction=\"You handle billing and payment-related inquiries.\",\n",
    "    description=\"Handles billing inquiries.\"\n",
    ")\n",
    "\n",
    "support_agent = LlmAgent(\n",
    "    name=\"Support\",\n",
    "    model=LiteLlm(model=MODEL_GPT_4O),\n",
    "    instruction=\"You provide technical support and troubleshooting assistance.\",\n",
    "    description=\"Handles technical support requests.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Coordinator Agent\n",
    "\n",
    "Now we'll create a coordinator agent that will route user requests to the appropriate specialized agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the coordinator agent\n",
    "coordinator = LlmAgent(\n",
    "    name=\"HelpDeskCoordinator\",\n",
    "    model=LiteLlm(model=MODEL_GPT_4O),\n",
    "    instruction=\"Route user requests: Use Billing agent for payment issues, Support agent for technical problems.\",\n",
    "    description=\"Main help desk router.\",\n",
    "    sub_agents=[billing_agent, support_agent]\n",
    ")\n",
    "\n",
    "# For ADK compatibility, the root agent must be named `root_agent`\n",
    "root_agent = coordinator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Runner\n",
    "\n",
    "The Runner is responsible for executing the agent and managing the conversation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(\n",
    "        app_name=\"test_agent\",\n",
    "        agent=root_agent,\n",
    "        artifact_service=InMemoryArtifactService(),\n",
    "        session_service=InMemorySessionService(),\n",
    "        memory_service=InMemoryMemoryService(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process a User Query with the Multi-Agent System\n",
    "\n",
    "Now let's simulate a user query and see how our multi-agent system handles it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:08:36 - LiteLLM:INFO\u001b[0m: utils.py:2870 - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n",
      "2025-05-14 09:08:36,440 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n",
      "2025-05-14 09:08:36,994 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:08:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "2025-05-14 09:08:37,005 - LiteLLM - INFO - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "\u001b[92m09:08:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "2025-05-14 09:08:37,010 - LiteLLM - INFO - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "\u001b[92m09:08:37 - LiteLLM:INFO\u001b[0m: utils.py:2870 - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n",
      "2025-05-14 09:08:37,011 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n",
      "2025-05-14 09:08:38,644 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:08:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "2025-05-14 09:08:38,649 - LiteLLM - INFO - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "2025-05-14 09:08:38,654 - <<MultiAgentTest>> - INFO - Event: HelpDeskCoordinator, Actions: skip_summarization=None state_delta={} artifact_delta={} transfer_to_agent=None escalate=None requested_auth_configs={}\n",
      "2025-05-14 09:08:38,655 - <<MultiAgentTest>> - INFO - Event: HelpDeskCoordinator, Actions: skip_summarization=None state_delta={} artifact_delta={} transfer_to_agent='Support' escalate=None requested_auth_configs={}\n",
      "2025-05-14 09:08:38,656 - <<MultiAgentTest>> - INFO - Event: Support, Actions: skip_summarization=None state_delta={} artifact_delta={} transfer_to_agent=None escalate=None requested_auth_configs={}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I can help you with that. When you are unable to log in to Gmail, there are a few troubleshooting steps you can try:\n",
      "\n",
      "1. Double-check your username and password: Ensure that you are entering the correct email address and password. Pay attention to capitalization and special characters.\n",
      "\n",
      "2. Clear your browser's cache and cookies: Sometimes stored data can cause login issues. Clearing the cache and cookies can help resolve this.\n",
      "\n",
      "3. Try using a different browser: If you are having trouble logging in, try using a different web browser to see if the issue persists.\n",
      "\n",
      "4. Check for any ongoing service outages: Sometimes Gmail may experience service disruptions. You can check the G Suite Status Dashboard for any reported issues.\n",
      "\n",
      "If you have tried these steps and are still unable to log in, please let me know so we can explore further options.\n"
     ]
    }
   ],
   "source": [
    "# Simulate a user query\n",
    "user_query = \"I can't log in gmail, help to give advice.\"\n",
    "# user_query = \"my billing is not working, help to give advice.\"\n",
    "user_id = \"test_user\"\n",
    "session_id = \"test_session\"\n",
    "\n",
    "# Create a session\n",
    "session = runner.session_service.create_session(\n",
    "    app_name=\"test_agent\",\n",
    "    user_id=user_id,\n",
    "    state={},\n",
    "    session_id=session_id,\n",
    ")\n",
    "\n",
    "# Create a content object with the user query\n",
    "content = types.Content(\n",
    "    role=\"user\", \n",
    "    parts=[types.Part.from_text(text=user_query)]\n",
    ")\n",
    "\n",
    "# Run the agent with the correct parameters\n",
    "events = list(runner.run(\n",
    "    user_id=user_id, \n",
    "    session_id=session.id, \n",
    "    new_message=content\n",
    "))\n",
    "\n",
    "# Process the events to get the response\n",
    "response = \"\"\n",
    "if events and events[-1].content and events[-1].content.parts:\n",
    "    for event in events:\n",
    "        logger.info(f\"Event: {event.author}, Actions: {event.actions}\")\n",
    "        response = \"\\n\".join([p.text for p in events[-1].content.parts if p.text])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different User Queries\n",
    "\n",
    "You can modify the user query in the cell below to test how the system routes different types of requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:08:42 - LiteLLM:INFO\u001b[0m: utils.py:2870 - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n",
      "2025-05-14 09:08:42,146 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: my billing is not working, help to give advice.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 09:08:42,752 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:08:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "2025-05-14 09:08:42,757 - LiteLLM - INFO - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "\u001b[92m09:08:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "2025-05-14 09:08:42,765 - LiteLLM - INFO - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "\u001b[92m09:08:42 - LiteLLM:INFO\u001b[0m: utils.py:2870 - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n",
      "2025-05-14 09:08:42,765 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n",
      "2025-05-14 09:08:43,332 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:08:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "2025-05-14 09:08:43,339 - LiteLLM - INFO - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "\u001b[92m09:08:43 - LiteLLM:INFO\u001b[0m: utils.py:2870 - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n",
      "2025-05-14 09:08:43,348 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I am here to help with your billing issue. Please provide me with more details about the problem you are facing with your billing.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Query: My application keeps crashing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 09:08:44,099 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:08:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "2025-05-14 09:08:44,103 - LiteLLM - INFO - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "\u001b[92m09:08:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "2025-05-14 09:08:44,107 - LiteLLM - INFO - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "\u001b[92m09:08:44 - LiteLLM:INFO\u001b[0m: utils.py:2870 - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n",
      "2025-05-14 09:08:44,108 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= gpt-3.5-turbo-0125; provider = openai\n",
      "2025-05-14 09:08:44,761 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:08:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n",
      "2025-05-14 09:08:44,764 - LiteLLM - INFO - selected model name for cost calculation: openai/gpt-3.5-turbo-0125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I am here to help with the crashing issue of your application. Could you provide more details about when the crashes occur and any error messages you see?\n"
     ]
    }
   ],
   "source": [
    "# Try a different query\n",
    "def process_query(query):\n",
    "    # Create a new session for each query\n",
    "    session_id = f\"test_session_{hash(query)}\"\n",
    "    \n",
    "    # Create a session\n",
    "    session = runner.session_service.create_session(\n",
    "        app_name=\"test_agent\",\n",
    "        user_id=user_id,\n",
    "        state={},\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    \n",
    "    # Create a content object with the user query\n",
    "    content = types.Content(\n",
    "        role=\"user\", \n",
    "        parts=[types.Part.from_text(text=query)]\n",
    "    )\n",
    "    \n",
    "    # Run the agent with the correct parameters\n",
    "    events = list(runner.run(\n",
    "        user_id=user_id, \n",
    "        session_id=session.id, \n",
    "        new_message=content\n",
    "    ))\n",
    "    \n",
    "    # Process the events to get the response\n",
    "    response = \"\"\n",
    "    if events and events[-1].content and events[-1].content.parts:\n",
    "        response = \"\\n\".join([p.text for p in events[-1].content.parts if p.text])\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example billing query\n",
    "billing_query = \"my billing is not working, help to give advice.\"\n",
    "print(f\"Query: {billing_query}\")\n",
    "print(f\"Response: {process_query(billing_query)}\")\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Example support query\n",
    "support_query = \"My application keeps crashing\"\n",
    "print(f\"Query: {support_query}\")\n",
    "print(f\"Response: {process_query(support_query)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
